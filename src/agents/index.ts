/**
 * Agent Implementations
 *
 * ‚úÖ Analyzer - —Ä–µ–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≤ LSM —á–µ—Ä–µ–∑ keyword matching
 * ‚úÖ Assembler - —Ä–µ–∞–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ LSM + raw_logs
 * ‚úÖ FinalResponder - —Ä–µ–∞–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ OpenAI —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø–∞–º—è—Ç–∏
 *
 * –í—Å–µ –∞–≥–µ–Ω—Ç—ã –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã.
 */

import { pool } from '../utils/db';
import { logger } from '../utils/logger';
import { createChatCompletion } from '../utils/openai';

/**
 * Helper: Sleep function
 */
function sleep(ms: number): Promise<void> {
  return new Promise(resolve => setTimeout(resolve, ms));
}

/**
 * Analyzer Agent - Memory Retriever
 *
 * –ó–∞–¥–∞—á–∞: –ù–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏–∑ LSM
 * –°—Ç–∞—Ç—É—Å—ã: NEW ‚Üí ANALYZING ‚Üí ANALYZED
 *
 * –¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è (v0.2):
 * - –ò–∑–≤–ª–µ–∫–∞–µ—Ç keywords –∏–∑ user_query
 * - –ò—â–µ—Ç –≤ lsm_storage —á–µ—Ä–µ–∑ semantic_tags && keywords (PostgreSQL array overlap)
 * - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ 3 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö memories
 * - –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ analysis_result
 *
 * TODO v0.3: Vector search –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
 */
export async function runAnalyzer(pipelineId: string): Promise<void> {
  logger.info(`[Analyzer] üîç Starting for ${pipelineId}`);

  try {
    // –ò–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω—ã–π –∑–∞—Ö–≤–∞—Ç –∑–∞–¥–∞—á–∏
    const result = await pool.query(
      `UPDATE pipeline_runs
       SET status = 'ANALYZING', updated_at = NOW()
       WHERE id = $1 AND status = 'NEW'
       RETURNING *`,
      [pipelineId]
    );

    if (result.rowCount === 0) {
      logger.warn(`[Analyzer] Task ${pipelineId} already taken or invalid status`);
      return;
    }

    const run = result.rows[0];
    logger.info(`[Analyzer] Processing query: "${run.user_query.substring(0, 50)}..."`);

    // Extract keywords from query
    const keywords = extractSimpleKeywords(run.user_query);
    logger.info(`[Analyzer] Extracted keywords: [${keywords.join(', ')}]`);

    // Search LSM for relevant memories (v0.1: keyword-based search)
    // Uses PostgreSQL array overlap operator (&&) to match semantic_tags
    const memoryResult = await pool.query(
      `SELECT summary_text, semantic_tags, time_bucket
       FROM lsm_storage
       WHERE user_id = $1
         AND semantic_tags && $2
       ORDER BY created_at DESC
       LIMIT 3`,
      [run.user_id, keywords]
    );

    const memories = memoryResult.rows.map(row => ({
      summary_text: row.summary_text,
      semantic_tags: row.semantic_tags,
      time_bucket: row.time_bucket
    }));

    logger.info(`[Analyzer] Found ${memories.length} memories from LSM (${keywords.length} keywords)`);

    // –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ (—Ñ–æ—Ä–º–∞—Ç –¥–ª—è Assembler)
    const analysis = {
      memories,
      search_keywords: keywords,
      timestamp: new Date().toISOString(),
    };

    // –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    await pool.query(
      `UPDATE pipeline_runs
       SET
         analysis_result = $1,
         status = 'ANALYZED',
         updated_at = NOW()
       WHERE id = $2`,
      [JSON.stringify(analysis), pipelineId]
    );

    logger.info(`[Analyzer] ‚úÖ Completed for ${pipelineId}`);
  } catch (error) {
    logger.error(`[Analyzer] ‚ùå Error for ${pipelineId}:`, error);
    throw error;
  }
}

/**
 * Extract simple keywords from query (v0.1 - basic implementation)
 *
 * TODO v0.2: Use OpenAI for better extraction
 * TODO v0.3: Use embeddings for semantic search
 */
function extractSimpleKeywords(query: string): string[] {
  // –ü—Ä–æ—Å—Ç–∞—è —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏—è: —É–±—Ä–∞—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞, –≤–∑—è—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ
  const stopWords = ['the', 'a', 'an', 'is', 'are', 'was', 'were', 'what', 'how', 'when', 'where', 'why', 'to', 'for', 'of', 'in', 'on', 'at'];

  const words = query.toLowerCase()
    .replace(/[^a-z0-9\s]/g, '')
    .split(/\s+/)
    .filter(w => w.length > 2 && !stopWords.includes(w));

  return [...new Set(words)]; // Unique words
}

/**
 * Assembler Agent - Context Builder v2
 *
 * –ó–∞–¥–∞—á–∞: –°–±–æ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è LLM —Å–æ–≥–ª–∞—Å–Ω–æ /context/format.md
 * –°—Ç–∞—Ç—É—Å—ã: ANALYZED ‚Üí ASSEMBLING ‚Üí READY
 *
 * –†–µ–∞–ª–∏–∑–∞—Ü–∏—è v2.0:
 * - –ß–∏—Ç–∞–µ—Ç analysis_result –æ—Ç Analyzer (memories –∏–∑ LSM)
 * - –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ—Ç memories –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ —Å–≤–µ–∂–µ—Å—Ç–∏
 * - –ß–∏—Ç–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∏ –∏–∑ raw_logs
 * - –ü—Ä–∏–º–µ–Ω—è–µ—Ç –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤ (~4000 —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
 * - –°–æ–±–∏—Ä–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç: SYSTEM ROLE + PREVIOUS CONTEXT (LSM) + RECENT CONVERSATION + CURRENT QUERY
 * - –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ final_context_payload –¥–ª—è FinalResponder
 */

// Constants for token management
const MAX_CONTEXT_TOKENS = 4000; // Leave room for response
const CHARS_PER_TOKEN = 4; // Rough estimate: 1 token ‚âà 4 chars

export async function runAssembler(pipelineId: string): Promise<void> {
  logger.info(`[Assembler] üì¶ Starting for ${pipelineId}`);

  try {
    // –ò–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω—ã–π –∑–∞—Ö–≤–∞—Ç
    const result = await pool.query(
      `UPDATE pipeline_runs
       SET status = 'ASSEMBLING', updated_at = NOW()
       WHERE id = $1 AND status = 'ANALYZED'
       RETURNING *`,
      [pipelineId]
    );

    if (result.rowCount === 0) {
      logger.warn(`[Assembler] Task ${pipelineId} already taken or invalid status`);
      return;
    }

    const run = result.rows[0];
    logger.info(`[Assembler] Building context for: "${run.user_query.substring(0, 50)}..."`);

    // –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ (–æ—Ç Analyzer)
    const analysis = run.analysis_result || { memories: [], search_keywords: [] };
    const searchKeywords = analysis.search_keywords || [];

    // –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å memories –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    const prioritizedMemories = prioritizeMemories(analysis.memories || [], searchKeywords);
    logger.info(`[Assembler] Prioritized ${prioritizedMemories.length} memories`);

    // –ü–æ–ª—É—á–∏—Ç—å recent conversation –∏–∑ raw_logs (—Ç–æ–ª—å–∫–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ)
    const logsResult = await pool.query(
      `SELECT
         log_type,
         log_data,
         created_at
       FROM raw_logs
       WHERE user_id = $1
         AND pipeline_run_id != $2
         AND processed = true
       ORDER BY created_at DESC
       LIMIT 10`,
      [run.user_id, pipelineId]
    );

    // –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ª–æ–≥–∏ –≤ –ø–∞—Ä—ã query-answer (reverse to chronological order)
    const allLogs = logsResult.rows.reverse();
    const recentLogs: Array<{ query: string; answer: string }> = [];
    for (let i = 0; i < allLogs.length; i += 2) {
      const queryLog = allLogs[i];
      const answerLog = allLogs[i + 1];

      if (queryLog && answerLog && queryLog.log_type === 'USER_QUERY' && answerLog.log_type === 'SYSTEM_RESPONSE') {
        recentLogs.push({
          query: queryLog.log_data.query,
          answer: answerLog.log_data.answer
        });
      }
    }

    logger.info(`[Assembler] Found ${recentLogs.length} recent exchanges from raw_logs`);

    // –°–æ–±—Ä–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —É—á—ë—Ç–æ–º –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤
    const context = buildContextWithTokenLimit(
      run.user_query,
      prioritizedMemories,
      recentLogs,
      MAX_CONTEXT_TOKENS
    );

    const estimatedTokens = Math.ceil(context.length / CHARS_PER_TOKEN);
    logger.info(`[Assembler] Context built: ${context.length} chars (~${estimatedTokens} tokens)`);

    // –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    await pool.query(
      `UPDATE pipeline_runs
       SET
         final_context_payload = $1,
         status = 'READY',
         updated_at = NOW()
       WHERE id = $2`,
      [context, pipelineId]
    );

    logger.info(`[Assembler] ‚úÖ Completed for ${pipelineId}`);
  } catch (error) {
    logger.error(`[Assembler] ‚ùå Error for ${pipelineId}:`, error);
    throw error;
  }
}

/**
 * Prioritize memories by relevance (tag overlap) and recency
 */
function prioritizeMemories(
  memories: Array<{ summary_text: string; semantic_tags?: string[]; time_bucket?: string }>,
  searchKeywords: string[]
): Array<{ summary_text: string; semantic_tags?: string[]; time_bucket?: string; score: number }> {
  if (!memories || memories.length === 0) return [];

  return memories
    .map(memory => {
      let score = 0;

      // Score by tag overlap
      if (memory.semantic_tags && searchKeywords.length > 0) {
        const tags = memory.semantic_tags.map(t => t.toLowerCase());
        const keywords = searchKeywords.map(k => k.toLowerCase());
        const overlap = tags.filter(t => keywords.some(k => t.includes(k) || k.includes(t))).length;
        score += overlap * 10;
      }

      // Score by recency (recent weeks get bonus)
      if (memory.time_bucket) {
        const match = memory.time_bucket.match(/(\d{4})-W(\d{2})/);
        if (match) {
          const year = parseInt(match[1]);
          const week = parseInt(match[2]);
          const now = new Date();
          const currentYear = now.getFullYear();
          const currentWeek = getWeekNumber(now);

          const weeksDiff = (currentYear - year) * 52 + (currentWeek - week);
          if (weeksDiff <= 1) score += 5; // This week or last week
          else if (weeksDiff <= 4) score += 3; // Last month
          else if (weeksDiff <= 12) score += 1; // Last 3 months
        }
      }

      return { ...memory, score };
    })
    .sort((a, b) => b.score - a.score);
}

/**
 * Get ISO week number
 */
function getWeekNumber(date: Date): number {
  const d = new Date(Date.UTC(date.getFullYear(), date.getMonth(), date.getDate()));
  const dayNum = d.getUTCDay() || 7;
  d.setUTCDate(d.getUTCDate() + 4 - dayNum);
  const yearStart = new Date(Date.UTC(d.getUTCFullYear(), 0, 1));
  return Math.ceil((((d.getTime() - yearStart.getTime()) / 86400000) + 1) / 7);
}

/**
 * Build context string with token limit
 */
function buildContextWithTokenLimit(
  currentQuery: string,
  memories: Array<{ summary_text: string; score?: number }>,
  recentLogs: Array<{ query: string; answer: string }>,
  maxTokens: number
): string {
  const maxChars = maxTokens * CHARS_PER_TOKEN;
  let context = '';
  let remainingChars = maxChars;

  // Section 1: SYSTEM ROLE (always included, ~200 chars)
  const systemRole = `SYSTEM ROLE:\nYou are a helpful AI assistant with long-term memory of past conversations with this user.\n\n`;
  context += systemRole;
  remainingChars -= systemRole.length;

  // Section 4: CURRENT QUERY (always included, reserve space)
  const querySection = `CURRENT QUERY:\n${currentQuery}\n\nPlease respond naturally, referencing past context when relevant.`;
  remainingChars -= querySection.length + 50; // Buffer

  // Section 2: PREVIOUS CONTEXT (from long-term memory) - prioritized
  if (memories && memories.length > 0) {
    let memorySection = `PREVIOUS CONTEXT (from long-term memory):\n`;
    let memoriesAdded = 0;

    for (const m of memories) {
      const memoryText = `‚Ä¢ ${m.summary_text}\n`;
      if (remainingChars - memoryText.length > 500) { // Keep buffer for conversations
        memorySection += memoryText;
        remainingChars -= memoryText.length;
        memoriesAdded++;
      } else {
        break;
      }
    }

    if (memoriesAdded > 0) {
      context += memorySection + '\n';
    }
  }

  // Section 3: RECENT CONVERSATION - limit to fit
  if (recentLogs && recentLogs.length > 0) {
    let convSection = `RECENT CONVERSATION:\n`;
    let conversationsAdded = 0;

    // Add most recent conversations first (they're most relevant)
    const recentFirst = [...recentLogs].reverse();
    const toAdd: string[] = [];

    for (const log of recentFirst) {
      const convText = `User: ${log.query}\nAssistant: ${log.answer}\n\n`;
      if (remainingChars - convText.length > 100) {
        toAdd.unshift(convText); // Add to beginning to maintain chronological order
        remainingChars -= convText.length;
        conversationsAdded++;
        if (conversationsAdded >= 3) break; // Max 3 recent conversations
      } else {
        break;
      }
    }

    if (conversationsAdded > 0) {
      context += convSection + toAdd.join('');
    }
  }

  // Add current query section
  context += querySection;

  return context;
}


/**
 * Final Responder Agent Stub
 *
 * –ó–∞–¥–∞—á–∞: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LLM
 * –°—Ç–∞—Ç—É—Å—ã: READY ‚Üí RESPONDING ‚Üí COMPLETED
 */
export async function runFinalResponder(pipelineId: string): Promise<void> {
  logger.info(`[FinalResponder] üí¨ Starting for ${pipelineId}`);

  try {
    // –ò–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω—ã–π –∑–∞—Ö–≤–∞—Ç
    const result = await pool.query(
      `UPDATE pipeline_runs
       SET status = 'RESPONDING', updated_at = NOW()
       WHERE id = $1 AND status = 'READY'
       RETURNING *`,
      [pipelineId]
    );

    if (result.rowCount === 0) {
      logger.warn(`[FinalResponder] Task ${pipelineId} already taken or invalid status`);
      return;
    }

    const run = result.rows[0];
    logger.info(`[FinalResponder] Generating response for: "${run.user_query.substring(0, 50)}..."`);

    // –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç Assembler (–µ—Å–ª–∏ –µ—Å—Ç—å)
    const contextPayload = run.final_context_payload || run.user_query;

    // –í—ã–∑–æ–≤ —Ä–µ–∞–ª—å–Ω–æ–≥–æ OpenAI
    logger.info('[FinalResponder] ü§ñ Calling OpenAI...');
    const answer = await createChatCompletion({
      model: 'gpt-4o-mini',
      messages: [
        {
          role: 'system',
          content: 'You are a helpful AI assistant with access to long-term memory. Provide clear, accurate, and contextual responses.'
        },
        {
          role: 'user',
          content: contextPayload
        }
      ],
      temperature: 0.7,
      max_tokens: 2000
    });

    logger.info(`[FinalResponder] ‚úÖ OpenAI responded (${answer.length} chars)`);

    // –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    await pool.query(
      `UPDATE pipeline_runs
       SET
         final_answer = $1,
         status = 'COMPLETED',
         updated_at = NOW()
       WHERE id = $2`,
      [answer, pipelineId]
    );

    // –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ raw_logs (–¥–ª—è –±—É–¥—É—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ Archivist)
    logger.info(`[FinalResponder] üìù Logging to raw_logs...`);

    // Log 1: USER_QUERY
    await pool.query(
      `INSERT INTO raw_logs (pipeline_run_id, user_id, log_type, log_data)
       VALUES ($1, $2, 'USER_QUERY', $3)`,
      [
        pipelineId,
        run.user_id,
        JSON.stringify({
          query: run.user_query,
          timestamp: new Date().toISOString()
        })
      ]
    );

    // Log 2: SYSTEM_RESPONSE
    await pool.query(
      `INSERT INTO raw_logs (pipeline_run_id, user_id, log_type, log_data)
       VALUES ($1, $2, 'SYSTEM_RESPONSE', $3)`,
      [
        pipelineId,
        run.user_id,
        JSON.stringify({
          answer: answer,
          timestamp: new Date().toISOString()
        })
      ]
    );

    logger.info(`[FinalResponder] ‚úÖ Logged 2 entries to raw_logs`);
    logger.info(`[FinalResponder] ‚úÖ Completed for ${pipelineId}`);
  } catch (error) {
    logger.error(`[FinalResponder] ‚ùå Error for ${pipelineId}:`, error);
    throw error;
  }
}

/**
 * Archivist Agent - Memory Creator
 *
 * –ó–∞–¥–∞—á–∞: –°–æ–∑–¥–∞—Ç—å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å (LSM) –∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤
 * –¢—Ä–∏–≥–≥–µ—Ä: –ü–æ—Å–ª–µ COMPLETED (–∏–ª–∏ –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é –¥–ª—è batch processing)
 *
 * –¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è (v1.0):
 * - –ß–∏—Ç–∞–µ—Ç raw_logs –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ pipeline_run
 * - –°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ—Ç –¥–∏–∞–ª–æ–≥ —á–µ—Ä–µ–∑ LLM
 * - –ò–∑–≤–ª–µ–∫–∞–µ—Ç semantic_tags —á–µ—Ä–µ–∑ LLM
 * - –ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –≤ lsm_storage
 * - –ü–æ–º–µ—á–∞–µ—Ç raw_logs –∫–∞–∫ processed
 */
export async function runArchivist(pipelineId: string): Promise<void> {
  logger.info(`[Archivist] üìö Starting for ${pipelineId}`);

  try {
    // 1. –ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ pipeline_run
    const pipelineResult = await pool.query(
      `SELECT user_id, user_query, final_answer
       FROM pipeline_runs
       WHERE id = $1 AND status = 'COMPLETED'`,
      [pipelineId]
    );

    if (pipelineResult.rowCount === 0) {
      logger.warn(`[Archivist] Pipeline ${pipelineId} not found or not completed`);
      return;
    }

    const pipeline = pipelineResult.rows[0];
    logger.info(`[Archivist] Processing dialog for user ${pipeline.user_id}`);

    // 2. –ß–∏—Ç–∞—Ç—å raw_logs –¥–ª—è —ç—Ç–æ–≥–æ pipeline (–µ—â—ë –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ)
    const logsResult = await pool.query(
      `SELECT id, log_type, log_data
       FROM raw_logs
       WHERE pipeline_run_id = $1
         AND processed = false
       ORDER BY created_at ASC`,
      [pipelineId]
    );

    if (logsResult.rowCount === 0) {
      logger.info(`[Archivist] No unprocessed logs for ${pipelineId}`);
      return;
    }

    const logs = logsResult.rows;
    logger.info(`[Archivist] Found ${logs.length} unprocessed logs`);

    // 3. –°–æ–±—Ä–∞—Ç—å –¥–∏–∞–ª–æ–≥ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
    const dialogText = logs.map(log => {
      if (log.log_type === 'USER_QUERY') {
        return `User: ${log.log_data.query}`;
      } else if (log.log_type === 'SYSTEM_RESPONSE') {
        return `Assistant: ${log.log_data.answer}`;
      }
      return '';
    }).filter(Boolean).join('\n\n');

    logger.info(`[Archivist] Dialog text: ${dialogText.length} chars`);

    // 4. –í—ã–∑–≤–∞—Ç—å LLM –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–≥–æ–≤
    logger.info(`[Archivist] ü§ñ Calling LLM for summarization...`);

    const archivistPrompt = `You are an archivist. Analyze this conversation and create a memory record.

CONVERSATION:
${dialogText}

Respond in JSON format with exactly these fields:
{
  "summary": "A 1-2 sentence summary of what was discussed, focusing on key facts and user preferences",
  "tags": ["tag1", "tag2", "tag3"] // 3-5 relevant keywords/topics as lowercase strings
}

Important:
- Summary should capture the essence of the conversation
- Tags should be useful for future retrieval (topics, entities, preferences mentioned)
- Keep tags simple and lowercase (e.g., "programming", "preferences", "typescript")`;

    const llmResponse = await createChatCompletion({
      model: 'gpt-4o-mini',
      messages: [
        { role: 'user', content: archivistPrompt }
      ],
      temperature: 0.3, // Lower temperature for more consistent JSON
      max_tokens: 500
    });

    logger.info(`[Archivist] LLM responded: ${llmResponse.length} chars`);

    // 5. –ü–∞—Ä—Å–∏—Ç—å JSON –æ—Ç–≤–µ—Ç
    let archiveData: { summary: string; tags: string[] };
    try {
      // –ò–∑–≤–ª–µ—á—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±—ë—Ä–Ω—É—Ç –≤ markdown)
      const jsonMatch = llmResponse.match(/\{[\s\S]*\}/);
      if (!jsonMatch) {
        throw new Error('No JSON found in response');
      }
      archiveData = JSON.parse(jsonMatch[0]);
    } catch (parseError) {
      logger.error(`[Archivist] Failed to parse LLM response:`, llmResponse);
      // Fallback: —Å–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—É—é –∑–∞–ø–∏—Å—å
      archiveData = {
        summary: `Dialog about: ${pipeline.user_query.substring(0, 100)}`,
        tags: extractSimpleKeywords(pipeline.user_query)
      };
    }

    logger.info(`[Archivist] Summary: "${archiveData.summary.substring(0, 80)}..."`);
    logger.info(`[Archivist] Tags: [${archiveData.tags.join(', ')}]`);

    // 6. –í—ã—á–∏—Å–ª–∏—Ç—å time_bucket (ISO week format: 2025-W47)
    const now = new Date();
    const timeBucket = getISOWeek(now);
    logger.info(`[Archivist] Time bucket: ${timeBucket}`);

    // 7. –ó–∞–ø–∏—Å–∞—Ç—å –≤ lsm_storage
    const logIds = logs.map(l => l.id);

    await pool.query(
      `INSERT INTO lsm_storage (user_id, time_bucket, semantic_tags, summary_text, source_run_ids)
       VALUES ($1, $2, $3, $4, $5)`,
      [
        pipeline.user_id,
        timeBucket,
        archiveData.tags,
        archiveData.summary,
        [pipelineId] // source_run_ids - –º–∞—Å—Å–∏–≤ UUID
      ]
    );

    logger.info(`[Archivist] ‚úÖ Created LSM record`);

    // 8. –ü–æ–º–µ—Ç–∏—Ç—å raw_logs –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
    await pool.query(
      `UPDATE raw_logs
       SET processed = true, processed_at = NOW()
       WHERE id = ANY($1)`,
      [logIds]
    );

    logger.info(`[Archivist] ‚úÖ Marked ${logIds.length} logs as processed`);
    logger.info(`[Archivist] ‚úÖ Completed for ${pipelineId}`);

  } catch (error) {
    logger.error(`[Archivist] ‚ùå Error for ${pipelineId}:`, error);
    throw error;
  }
}

/**
 * Get ISO week string (e.g., "2025-W47")
 */
function getISOWeek(date: Date): string {
  const d = new Date(Date.UTC(date.getFullYear(), date.getMonth(), date.getDate()));
  const dayNum = d.getUTCDay() || 7;
  d.setUTCDate(d.getUTCDate() + 4 - dayNum);
  const yearStart = new Date(Date.UTC(d.getUTCFullYear(), 0, 1));
  const weekNo = Math.ceil((((d.getTime() - yearStart.getTime()) / 86400000) + 1) / 7);
  return `${d.getUTCFullYear()}-W${weekNo.toString().padStart(2, '0')}`;
}
