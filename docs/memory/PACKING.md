# PACKING: Алгоритм семантической упаковки

> **Статус:** v0.1 — постановка проблемы
> **Основной документ:** [LSM.md](./LSM.md)
> **Связь:** Используется в LSM при переходах Hot→Warm→Cold

---

## 1. Проблема

### Что такое упаковка

Упаковка (packing) — это преобразование сырого диалога в компактное представление, которое:
- Занимает меньше места (токенов)
- Сохраняет ключевую информацию
- Позволяет восстановить контекст при необходимости

### Почему это сложно

**Наивный подход не работает:**
```
Промпт: "Сумморизируй этот диалог"
→ LLM выдаёт generic summary
→ Теряются критичные детали
→ При восстановлении невозможно ответить на конкретные вопросы
```

**Корневые причины:**

1. **LLM не понимает контекст пользователя**
   - Видит только текст диалога
   - Не знает, что важно для этого конкретного человека
   - Не знает историю взаимодействий

2. **Потеря связей**
   - Диалог может ссылаться на прошлые обсуждения
   - При упаковке эти связи теряются
   - "Как мы решили в прошлый раз" → непонятно, о чём речь

3. **Неправильная приоритизация**
   - LLM не различает: решение vs обсуждение vs small-talk
   - Может сохранить "Привет, как дела?" и потерять "Утвердили бюджет 100К"

4. **Разные цели на разных слоях**
   - Hot→Warm: сохранить возможность детального восстановления
   - Warm→Cold: сохранить только навигационную информацию
   - Один алгоритм не подходит для обоих случаев

---

## 2. Требования

### Функциональные

| # | Требование | Критерий успеха |
|---|------------|-----------------|
| F1 | Сохранение сущностей | 100% ключевых сущностей восстанавливаются |
| F2 | Сохранение решений | Все принятые решения можно извлечь из саммари |
| F3 | Сохранение связей | Ссылки на другие эпизоды сохраняются |
| F4 | Разные алгоритмы для разных переходов | Hot→Warm ≠ Warm→Cold |

### Нефункциональные

| # | Требование | Критерий успеха |
|---|------------|-----------------|
| N1 | Коэффициент сжатия | Hot→Warm: 3-5x, Warm→Cold: 10-20x |
| N2 | Восстанавливаемость | По саммари можно ответить на 80%+ вопросов |
| N3 | Тестируемость | Качество измеряется автоматически |

### Ключевой принцип

> **Саммаризация — это не сжатие текста. Это интеграция нового знания в существующую модель мира пользователя.**

---

## 3. Метрики качества

### Pack → Unpack → Compare

Основной метод оценки — как lossy compression:

```
1. Original: взять реальный диалог
2. Pack: упаковать (Archivist)
3. Questions: сгенерировать 5-10 вопросов по оригиналу
4. Unpack: попытаться ответить ТОЛЬКО из саммари
5. Compare: сравнить ответы с эталонными
6. Score: % правильных ответов
```

### Конкретные метрики

| Метрика | Что измеряем | Целевое значение |
|---------|--------------|------------------|
| **Entity Recall** | % сущностей, которые восстановились | >95% |
| **Decision Preservation** | % решений, которые можно извлечь | 100% |
| **Question Answerability** | % вопросов, на которые можно ответить | >80% |
| **Semantic Similarity** | Косинусное расстояние эмбеддингов | >0.85 |
| **Compression Ratio** | Размер саммари / Размер оригинала | 0.2-0.3 |

---

## 4. Соображения по решению

### Гипотеза: упаковка как мини-пайплайн

Вместо одного промпта — последовательность шагов:

```
Сырой диалог
    ↓
[1. Entity Extractor] → сущности (люди, проекты, технологии, решения)
    ↓
[2. Linker] → связи с существующими эпизодами в LSM
    ↓
[3. Novelty Scorer] → что НОВОЕ vs повторение известного?
    ↓
[4. Summary Generator] → саммари с учётом всего выше
    ↓
[5. Validator] → проверка качества (можно ли ответить на вопросы?)
```

### Почему пайплайн, а не один промпт

1. **Декомпозиция задачи** — каждый шаг делает одно дело хорошо
2. **Контекст для LLM** — Summary Generator видит уже извлечённые сущности и связи
3. **Отладка** — можно понять, на каком шаге теряется информация
4. **Итерация** — можно улучшать каждый шаг независимо

### Разные алгоритмы для разных переходов

**Hot → Warm:**
- Цель: сохранить возможность детального восстановления
- Сохраняем: summary + key_points + entities + decisions + embedding
- Ссылка на raw_logs остаётся

**Warm → Cold:**
- Цель: оставить только навигационную информацию
- Сохраняем: headline (1 предложение) + tags + embedding
- Достаточно для решения "копать глубже или нет"

---

## 5. Интеграция с Self-Learning

Упаковка — идеальный кандидат для самообучения:

```
Emulator → генерит тестовые диалоги
    ↓
Archivist → пакует (текущий алгоритм)
    ↓
Analyst → оценивает качество упаковки (метрики выше)
    ↓
Teacher → предлагает улучшения промптов
    ↓
Tuner → применяет изменения
    ↓
[Повтор цикла]
```

**Единственный импакт:** промпт(ы) в пайплайне упаковки.

---

## 6. Открытые вопросы

1. **Сколько LLM-вызовов в пайплайне?**
   - Можно ли объединить шаги для экономии?
   - Или качество важнее стоимости?

2. **Как работает Linker?**
   - Поиск по эмбеддингам в LSM?
   - Какой порог similarity для "связанных" эпизодов?

3. **Как генерировать тестовые вопросы?**
   - Автоматически из диалога?
   - Или нужен отдельный агент?

4. **Валидатор как gate?**
   - Если качество < порога, что делать?
   - Повторить с другим промптом? Сохранить больше сырых данных?

---

*Версия: 0.1*
*Дата: 2025-11-30*
