Это черновик архитектурного документа (White Paper), объединяющий твою философию с технической реализацией. Документ структурирован так, чтобы его можно было использовать как ТЗ для разработчиков или дорожную карту для самостоятельной разработки.

---

# Архитектура Памяти ИИ-Ассистента: Принцип Анизотропии
**Версия документа:** 1.0
**Цель:** Создание системы памяти, превосходящей человеческую за счет рационального управления актуальностью и целостностью данных.

---

## 1. Концептуальный Базис (Философия Архитектуры)

В основе архитектуры лежит принцип **анизотропности памяти**: качество, скорость доступа и детализация информации зависят не столько от хронологии, сколько от **актуальности** (приоритета) для текущей задачи.

Система имитирует когнитивный процесс человека:
1.  **Фокус внимания:** Работа с текущим контекстом происходит с максимальной детализацией («на кончиках пальцев»).
2.  **Естественное затухание:** Решенные или неактуальные задачи «упаковываются» в сжатые смысловые слепки.
3.  **Динамическая распаковка:** При необходимости старый контекст не просто «считывается», а переносится в зону актуальности (Re-Actualization), восстанавливая целостность связей.

---

## 2. Структурные Слои Памяти

Система делится на три зоны хранения, различающиеся по плотности информации и скорости доступа.

### Уровень 1: Горячая Память (Hot Memory / The Focus)
*Аналог: Краткосрочная память + Оперативная память.*

* **Назначение:** Обеспечение мгновенной реакции и удержание полного контекста текущей дискуссии.
* **Временной горизонт:** Динамический (по умолчанию ~2 недели или N-последних токенов активного диалога).
* **Формат хранения:** Raw Text (сырой, необработанный текст диалогов).
* **Характеристики:**
    * **Максимальная целостность:** Хранятся все нюансы, интонации и промежуточные шаги.
    * **Scale-Free Connectivity:** Высокая плотность связей между фактами внутри этого окна.
* **Техническая реализация:** Контекстное окно LLM + Быстрый кэш (Redis/In-memory DB). Данные подаются в модель *всегда* при каждом запросе.

### Уровень 2: Упакованная Память (Packed Storage / The Index)
*Аналог: Ассоциативная память, «обрывки воспоминаний».*

* **Назначение:** Быстрый поиск («О чем это было?») и принятие решения о необходимости глубокого восстановления.
* **Формирование:** Запускается, когда данные вытесняются из «Горячей памяти».
* **Формат хранения:**
    * **Саммари (Summaries):** Сжатое описание сути разговора, ключевых решений и фактов.
    * **Эмбеддинги (Embeddings):** Векторное представление смысла для семантического поиска.
    * **Метаданные:** Тэги, даты, участники, статус задачи (Решена/Не решена).
* **Техническая реализация:** Векторная база данных (Pinecone, Weaviate, Milvus).

### Уровень 3: Холодный Архив (Cold Vault / The Source)
*Аналог: Долговременная память, «библиотека книг».*

* **Назначение:** Хранение исходных данных для возможной «распаковки» в будущем.
* **Формат хранения:** Полные логи (Raw Logs), структурированные по времени и сессиям, связанные ID с «Упакованной памятью».
* **Техническая реализация:** Дешевое объектное хранилище (S3) или NoSQL база (MongoDB). Доступ сюда происходит *только* по специальному запросу системы.

---

## 3. Алгоритмы Работы (Process Flow)

Главное отличие от стандартных RAG-систем — двухступенчатый механизм восстановления контекста.

### А. Процесс "Упаковки" (Архивация)
Когда сессия закрывается или истекает срок актуальности (Time-to-Live горячего окна):
1.  **Анализ:** Агент-архивариус читает «сырой» диалог.
2.  **Сжатие:** Генерируется саммари (минимальное семантическое поле).
3.  **Индексация:** Саммари превращается в вектор для поиска.
4.  **Связывание:** Вектор получает ссылку (ID) на полный текст в Холодном Архиве.
5.  **Очистка:** Сырой текст удаляется из Горячей памяти.

### Б. Процесс "Вспоминания" (Retrieval Protocol)
Когда пользователь задает вопрос, выходящий за рамки текущего разговора:

**Шаг 1: Быстрый скан (Symetric Search)**
* Система ищет по векторной базе (Уровень 2).
* Находит релевантные «упакованные» фрагменты (саммари).
* **Внутренний монолог системы:** «Я нашел воспоминания о проекте Х полугодовой давности. В саммари сказано, что мы обсуждали архитектуру. Достаточно ли этого для ответа на текущий вопрос?»

**Шаг 2: Ветвление (Decision Making)**
* **Сценарий «Поверхностный»:** Если вопрос общий («А мы говорили про Х?»), система отвечает, используя *только* саммари.
    * *Результат:* Экономия ресурсов, быстрый ответ. Контекст остается в прошлом.
* **Сценарий «Глубокое восстановление»:** Если вопрос требует деталей («Какие именно параметры мы утвердили для Х?»), система инициирует процедуру распаковки.

**Шаг 3: Распаковка и Реактуализация (Unpacking & Re-Actualization)**
* Система берет ID из найденного саммари.
* Извлекает полный текст из Холодного Архива (Уровень 3).
* **Инъекция:** Этот старый текст загружается в Горячую Память (Уровень 1) *текущей сессии*.
* **Эффект:** Старая информация теперь считается «новой» и актуальной. Модель видит её в полном объеме связей.

---

## 4. Ключевые метрики успеха (KPIs архитектуры)

1.  **Коэффициент «Шум/Сигнал»:** Система не должна тянуть старый контекст без необходимости (не «замусоривать» промпт).
2.  **Скорость «Вспоминания»:** Время от запроса до решения о распаковке должно быть минимальным.
3.  **Целостность контекста:** После распаковки модель должна отвечать так, будто разговор был 5 минут назад, а не полгода.

---

## 5. Технический стек (Рекомендация для MVP)

Для реализации «чтобы работало» предлагается следующая связка:

* **LLM Core:** GPT-4o или Claude 3.5 Sonnet (лучше работают с контекстом).
* **Orchestrator:** LangChain или LangGraph (для управления логикой «решить/распаковать»).
* **Hot Memory:** Redis (хранение сессии диалога).
* **Vector DB (Packed):** ChromaDB или Pinecone (хранение саммари и эмбеддингов).
* **Cold Storage:** JSON-файлы на диске или PostgreSQL (полные логи).

---

### Следующий шаг
Нужно определить **Сценарий №1** для теста.
*Вариант:* Создать бота, поговорить с ним о книге, подождать (симулировать) 2 недели, а потом спросить мелкую деталь сюжета, чтобы проверить, как он «распакует» архив.

Готов переходить к написанию псевдокода для этого сценария?